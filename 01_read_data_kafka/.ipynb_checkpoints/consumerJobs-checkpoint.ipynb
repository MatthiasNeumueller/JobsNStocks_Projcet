{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfd602-cbd0-494e-a425-bd64c27a68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "from json import loads\n",
    "import pymongo\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c916321-b09a-4f96-897a-4224d85e5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to MogoDB\n",
    "client = MongoClient('mongodb+srv://attila:JS8WMhoQB65LynxB@cluster0.gu4ru.mongodb.net', ssl_cert_reqs=ssl.CERT_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941ece9-9a30-4ef5-9091-578d356c0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = client.list_database_names()\n",
    "if \"Group_B1_JobStock_db\" in dbs:\n",
    "    if \"jobs\" in Group_B1_JobStock_db.list_collection_names():\n",
    "        mycol.drop() \n",
    "else:\n",
    "    Group_B1_JobStock_db = client[\"Group_B1_JobStock_db\"]\n",
    "    print(\"Mongo DB created\")\n",
    "    \n",
    "mycol = Group_B1_JobStock_db[\"jobs\"]\n",
    "print(\"Collection jobs initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1ba8d98-290d-4428-98f6-12142a966f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobs']\n"
     ]
    }
   ],
   "source": [
    "#dbs = Group_B1_JobStock_db.list_collection_names()\n",
    "#print(dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00acf2a7-c235-4346-ad4e-1fa20c533f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'numtest',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "850239ed-8ea5-4efb-941f-69829629f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"Junior Data Analyst - Machine Learning (w/m/d)\", \"company_name\": \"KERN engineering careers GmbH\", \"location\": \"Wels, \\u00d6sterreich\", \"via\": \"\\u00fcber Jobsuche - DerStandard\", \"description\": \"\\ufeff\\nJunior Data Analyst - Machine Learning (w/m/d)\\n\\nDaten sind das neue Roh\\u00f6l! #MehrwertausDaten\\n\\n10.12.2021\\n\\n\\u2003\\n\\nJob-Nr.: 169095\\n\\nUnser Klient z\\u00e4hlt zu den internationalen Vorzeigeunternehmen in der chemischen Industrie. Bis heute im Familienbesitz, \\u00fcberzeugt das Unternehmen weltweite Kunden mit traditionellen Werten, wie hohen Qualit\\u00e4tsanspr\\u00fcchen und Zuverl\\u00e4ssigkeit. Die Vorreiterrolle im Markt verdankt man gleicherma\\u00dfen dem st\\u00e4ndigen Blick nach vorne - Themen wie Nachhaltigkeit und Digitalisierung sind hier mehr als nur leere Floskeln.\\n\\nGro\\u00dfraum Wels\\n\\nSoftwareentwicklung\\n\\nIhr Aufgabengebiet\\n\\u2022 Als Spezialist auf deinem Gebiet entwirrst Du den internen Datenpool und generierst damit einen erheblichen Mehrwert f\\u00fcr die Abteilungen\\n\\u2022 Gemeinsam mit deinen Teamkollegen stehst Du an vorderster Front bei der Umsetzung internationaler Projekte im Bereich Digitalisierung\\n\\u2022 Du leitest eigene Handlungsempfehlungen ab und stehst als gefragte/r Ansprechpartner/in in engem Kontakt zu den internen... Fachabteilungen und globalen Expertenteams\\n\\u2022 Du verwaltest das hauseigene Data-Warehouse und bedienst Dich neuester Methoden wie Machine Learning\\n\\u2022 Du entscheidest selbst wie du die Daten f\\u00fcr die Fachbereiche aufbereitest und visualisierst\\n\\u2022 Statistische Verfahren anwenden und Modelle erstellen geh\\u00f6ren zu deinen t\\u00e4glichen Aufgaben\\n\\nUnsere Anforderungen\\n\\u2022 Du hast eine abgeschlossene Ausbildung (FH, Universit\\u00e4t) im Bereich Data Science, Statistik, Informatik, Mathematik o.\\u00c4.\\n\\u2022 Du bringst erste einschl\\u00e4gige Berufserfahrung mit und findest Dich in komplexen Datenstrukturen gut zurecht\\n\\u2022 Erfahrungen mit g\\u00e4ngigen Technologien wie Python, R, Scala, Azure heben Dich besonders hervor\\n\\u2022 Du sprichst ausgezeichnetes Deutsch und flie\\u00dfendes Englisch\\n\\u2022 Eine strukturierte, analytische Denkweise, sowie eine kommunikative Pers\\u00f6nlichkeit zeichnen Dich aus\\n\\nWas wir bieten\\n\\u2022 M\\u00f6glichkeit durch erfahrene Kollegen an deiner Seite tiefer in den Bereich Data Science einzutauchen\\n\\u2022 Angesehene Expertenrolle im Konzern und wichtige/r Ansprechpartner/in f\\u00fcr die Fachabteilungen\\n\\u2022 Raum f\\u00fcr eigene Ideen und kreative L\\u00f6sungen\\n\\u2022 Abwechslungsreiche Herausforderungen bei der Begleitung globaler Digitalisierungsprojekte in einem internationalen Kontext\\n\\u2022 Dynamisches Umfeld sowie ein gro\\u00dfes Netzwerk an weltweiten Kontakten\\n\\u2022 Offene Unternehmenskultur und ausgepr\\u00e4gte Teamorientierung\\n\\u2022 M\\u00f6glichkeit zur beruflichen und pers\\u00f6nlichen Weiterentwicklung\\n\\u2022 Attraktives Gehaltspaket und Zusatzleistungen (inkl. Home-Office)\\n\\nIhr Gehalt\\n\\nMindestgehalt\\n\\nEUR 3200 brutto / Monat (auf Vollzeitbasis)\\n\\nIhr M\\u00f6gliches Gehalt\\n\\nSie erhalten zwischen EUR 3200 und EUR 4200 brutto / Monat - abh\\u00e4ngig von Ihrer Qualifikation und Erfahrung mit der Bereitschaft zur \\u00dcberbezahlung.\\n\\nJetzt Bewerben\\n\\nWenn Sie in dieser Position eine Herausforderung sehen, bewerben Sie sich online. Die zust\\u00e4ndige Ansprechpartnerin, Marlies Teichmann,\\nm.teichmann@kern-partner.at, wird sich umgehend mit Ihnen in Verbindung setzen\", \"extensions\": [\"vor 18 Stunden\", \"3200\\u00a0\\u20ac pro Monat\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 18 Stunden\", \"schedule_type\": \"Vollzeit\", \"salary\": \"3200\\u00a0\\u20ac pro Monat\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJKdW5pb3IgRGF0YSBBbmFseXN0IC0gTWFjaGluZSBMZWFybmluZyAody9tL2QpIiwiaHRpZG9jaWQiOiJvbnVQRDdRaUJrWEtWLWVMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJSFFYVnpkSEpwWVEiLCJobCI6ImRlIiwiYXBwbHlfbGluayI6eyJ0aXRsZSI6IkJld2VyYnVuZyBhdWY6IEpvYnN1Y2hlIC0gRGVyU3RhbmRhcmQiLCJsaW5rIjoiaHR0cHM6Ly9qb2JzLmRlcnN0YW5kYXJkLmF0L2pvYnN1Y2hlL2pvYnMvMzIxODIyL2p1bmlvci1kYXRhLWFuYWx5c3QtbWFjaGluZS1sZWFybmluZy13LW0tZD91dG1fY2FtcGFpZ249Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fc291cmNlPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX21lZGl1bT1vcmdhbmljIn19\"}\n",
      "{\"title\": \"DATA LABELING ENGINEER (M/F/D)\", \"company_name\": \"i5invest\", \"location\": \"Graz, \\u00d6sterreich\", \"via\": \"\\u00fcber Torre.co\", \"description\": \"One of our portfolio companies has developed an intelligent audio algorithm using a combination of AI and signal processing to create an automatic audio post production web service - a kind of audio autopilot for podcasts, broadcasters, radio, movies, audiobooks, lecture recordings, and more.\\n\\nWe are looking for support to shape the future of audio algorithms and our audio web service!\\n\\nHundreds of thousands rely on our services and algorithms, which keeps adapting to new data every day.\\n\\nOur portfolio company encourages you to try your ideas and help the algorithm to learn quickly, to discover new technologies and get the learning for the next stage of evolving and growing.\\n\\nYou will create new records, in close cooperation with our Machine Learning Developers, and will support the audio algorithm to train.\\n\\nYour tasks:\\n\\n\\\\- Processing and filtering of audio data comming from the web service\\n\\n\\\\- Reviewing and sorting of external data records\\n\\n\\\\- Post-processing and rating of results... comming from the algorithm (according to subjective listening tests)\\n\\nYour Qualifications:\\n\\n\\\\- An (almost) completed Bachelor\\\\/Master Degree in Coomputer Science or similar\\n\\n\\\\- First experience with audio software such as Audacity or Reaper\\n\\n\\\\- Curiosity for audio, music and podcasts\\n\\n\\\\- Relevant Linux experience or Bash\\\\/Phyton scripting would be a nice but isn't a must\\n\\n\\\\- Self-motivated, ability to learn quickly and adapt to changes\\n\\nWhat the company ofers:\\n\\n\\\\- Working in a fast changing enviroment\\n\\n\\\\- New and interesting tasks\\n\\n\\\\- Part-time contract (15-25h\\\\/week)\\n\\n\\\\- Great office in Graz (city center) and an outstanding team\\n\\n\\\\- Flexible working culture with Homeoffice posibility\\n\\nUbicaci\\u00f3n: Graz, AT\\n\\nHabilidades requeridas para este trabajo:\\n\\u2022 REST\\n\\u2022 Linux\", \"thumbnail\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcResZbUIsmMMepbRYl9azj660vpRAlvFw7YzB16CU4&s\", \"extensions\": [\"vor 1 Tag\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 1 Tag\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEQVRBIExBQkVMSU5HIEVOR0lORUVSIChNL0YvRCkiLCJodGlkb2NpZCI6IlpiSjAzcnE4VUI4NlBXczJBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lIUVhWemRISnBZUSIsImhsIjoiZGUiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiQmV3ZXJidW5nIGF1ZjogVG9ycmUuY28iLCJsaW5rIjoiaHR0cHM6Ly90b3JyZS5jby9qb2JzLzhXMzAxZ1J3P3V0bV9jYW1wYWlnbj1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9zb3VyY2U9Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fbWVkaXVtPW9yZ2FuaWMifX0=\"}\n",
      "{\"title\": \"Data Scientist Engineer (m/w/d)\", \"company_name\": \"Antal International\", \"location\": \"Bregenz, \\u00d6sterreich\", \"via\": \"\\u00fcber Energy Jobline\", \"description\": \"We are looking for a to join our team and to help taking our business to the next level! Data Scientist Engineer (m/w/d) These are your responsibilities \\u2013 Build ETL pipelines in Python to extract data from various data sources including external APIs, parse, clean, and load into optimized database tables \\u2013 Design high-performance database schemas and optimize SQL queries \\u2013 Investigate data problems and identify patterns \\u2013 Draw inferences and conclusions, communicate results through reports, charts, or tables \\u2013 Work closely with business stakeholders for reporting, data analysis, and ad-hoc data investigations Qualifications This is you \\u2013 You have 2+ years relevant experience in Data Engineering \\u2013 You are a pro in Python programming \\u2013 You are an expert in SQL and database design and bring a good knowledge of data structures, algorithms, and statistics \\u2013 You feel comfortable with the AWS ecosystem (S3, Redshift, EC2) \\u2013 You have experience with Google Analytics, Google Big Query, Google... Tag Managerbuilding ETL pipelines, Unix shell scripting (the workflow management tool Airflow is plus) \\u2013 You have an analytical mind and bake your decisions with data \\u2013 You are involved in the whole process of ML model development. This includes everything from root cause analysis, data-mining and feature engineering to training, validating and implementing machine learning models, computing performance statistics and conducting A/B-tests. \\u2013 You are fluent in English (both spoken and written) and a strong communicator Additional information What we offer you \\u2013 International colleagues and a great work environment \\u2013 Challenges and room for professional and personal development \\u2013 Responsibilities from day one and flat hierarchies \\u2013 Flexible working hours\", \"thumbnail\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTu4zJElra8gQ_CSG1hk4f6w92IMhUFdogHcZIvufY&s\", \"extensions\": [\"vor 2 Tagen\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 2 Tagen\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBFbmdpbmVlciAobS93L2QpIiwiaHRpZG9jaWQiOiJmUGxXLWQwTnpmRDBZS0ZMQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJSFFYVnpkSEpwWVEiLCJobCI6ImRlIiwiZmMiOiJFdmNCQ3JjQlFVMXNkbkp3ZEZGRU9EaGtNMHBGV1ZsalpGRk9aMk5JWm5OcmF6TkpMVk50ZEVOZk1XUmFURjlyVURCWVlVZzFORWRIYlU1d2NuQnBSVXc1T0VReVZqTTBaRFpxVFZoVmEwTm1TWE5mWW1WQ1JWVTNPRVEwVURZMlExZG5aekpVYmpSTGJrdHZOVVZRY1dNeVRHdG9ORXh4VDNFMExWOUtVVVJ1WW1Jd1VYZHlVblJpVEhCT1JuSmpVbkJSY2w5b2FYbzBMVTg1TUhacFJsTklVbWcxVld4V2NVaFRXRVpMT1hWNlIzTmlaRUZvZEhCVE9UUlJFaGR4YjA4eldXWmhRMEZpUjFFNWRUaFFkemRwZFhWQk5Cb2lRVWhYVTA1dFdFSkNOa1V6VDJ4NVMzY3pPRVp2YjJ4eGVEWXRabkJKTkdsUFp3IiwiZmN2IjoiMyIsImZjX2lkIjoiZmNfMzEiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiQmV3ZXJidW5nIGF1ZjogRW5lcmd5IEpvYmxpbmUiLCJsaW5rIjoiaHR0cHM6Ly93d3cuZW5lcmd5am9ibGluZS5jb20vam9iL2RhdGEtc2NpZW50aXN0LWVuZ2luZWVyLW0tdy1kLTkwNjkwNjI/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5pYyJ9fQ==\"}\n",
      "{\"title\": \"Senior Critical Environment Technician\", \"company_name\": \"Microsoft\", \"location\": \"Wien, \\u00d6sterreich\", \"via\": \"\\u00fcber Microsoft Careers\", \"description\": \"In alignment with our Microsoft values, we are committed to cultivating an inclusive work environment for all employees to positively impact our culture every day and we need you as a Senior Data Center Critical Environment Technician (CET IV).\\n\\nMicrosoft\\u2019s Cloud Operations & Innovation (CO+I) is the engine that powers our cloud services. As a CO+I Critical Environment Technician, you will perform a key role in delivering the core infrastructure and foundational technologies for Microsoft's online services including Bing, Office 365, Xbox, OneDrive, and the Microsoft Azure platform. As a group, CO+I is focused on the personal and professional development of all employees and offers trainings and opportunities including Career Rotation Programs, Diversity & Inclusion trainings and events, and professional certifications.\\n\\nOur infrastructure is comprised of a large global portfolio of more than 100 datacenters and 1 million servers. Our foundation is built upon and managed by a team of... subject matter experts working to support services for more than 1 billion customers and 20 million businesses in over 90 countries worldwide.\\n\\nWith environmental sustainability and optimization at the forefront of our datacenter design and operations, we continue to grow and evolve as we meet the ever-changing business demands that hold Microsoft as a world-class cloud provider.\\n\\nDo you want to empower billions across the world? Come and join us in CO+I and be at the forefront of the action!\\n\\n#COICareers #Buildthecloud #ChangetheWorld #WeSeeYou #RepresentationMatters\", \"thumbnail\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT8hW3BqNcBc5AYVS8Wwr6jx7tHESt6fymqH0T3G8o&s\", \"extensions\": [\"vor 3 Tagen\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 3 Tagen\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJTZW5pb3IgQ3JpdGljYWwgRW52aXJvbm1lbnQgVGVjaG5pY2lhbiIsImh0aWRvY2lkIjoiZGh0MWlONzVyREE3ek9VSUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUhRWFZ6ZEhKcFlRIiwiaGwiOiJkZSIsImZjIjoiRXN3QkNvd0JRVTFzZG5Kd2RHeGhjR2gxYm5BdGJEQXdTR3BUTlhoSk4wNHdXRE5KWTBOTFozRjJjRk5CWlcxTlVHeDJVM2RpU25SeldWRnhNMjVpWjJWRVdIZGpNVzVoUXpkQlkyUnFkM05wTTE4d1lqWjZaMEZrYmxsTVFWWldTR3BpT0hsU1oyazVOMjF3YVdkd1gwTnFVVWs1WDNVeVZrMDBkR0ZuVlZKdGRsaG9ZV3Q2YldGTmNWRldRVU52ZEVJU0YzRnZUek5aWm1GRFFXSkhVVGwxT0ZCM04ybDFkVUUwR2lKQlNGZFRUbTFZY2xoa1UzZFliazU2VTI1S05XRlhORGs0ZW5CcU5rWnNiRWxCIiwiZmN2IjoiMyIsImZjX2lkIjoiZmNfNDAiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiQmV3ZXJidW5nIGF1ZjogTWljcm9zb2Z0IENhcmVlcnMiLCJsaW5rIjoiaHR0cHM6Ly9jYXJlZXJzLm1pY3Jvc29mdC5jb20vcHJvZmVzc2lvbmFscy91cy9lbi9qb2IvMTIyMjM2NC9TZW5pb3ItQ3JpdGljYWwtRW52aXJvbm1lbnQtVGVjaG5pY2lhbj91dG1fY2FtcGFpZ249Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fc291cmNlPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX21lZGl1bT1vcmdhbmljIn19\"}\n",
      "{\"title\": \"Data Scientist\", \"company_name\": \"Tractive\", \"location\": \"Pasching, \\u00d6sterreich\", \"via\": \"\\u00fcber Tractive\", \"description\": \"Full-time (Linz, Austria)\\nStarting now!\\nYour territory\\n\\nAs part of our Data team, you will:\\n\\u2022 Fuel the development of better products and business strategies by mining and analyzing data\\n\\u2022 Identify opportunities for leveraging company data in collaboration with other teams\\n\\u2022 Develop custom data models and algorithms to apply to data sets\\n\\u2022 Develop processes and tools to monitor and analyze data\\n\\u2022 Go deeper into advanced analytics with machine learning and AI solutions\\n\\u2022 Take ownership of one-off projects that use data to spark success\\n\\u2022 Bring in your fresh ideas to make Tractive better - you\\u2019ll never hear the phrase \\u201c...because that\\u2019s how we\\u2019ve always done things\\u201d.\\n\\u2022 Continuously grow personally and professionally, take ownership of areas that show your potential, and attend workshops which help you get to the next level.\\n\\nYour profileKey requirements:\\n\\u2022 Good knowledge of SQL, Python and R\\n\\u2022 Experience with relational databases (e.g. Amazon Redshift) and document-oriented databases... (MongoDB)\\n\\u2022 Knowledge in data engineering, data modelling and experience with ETL processes\\n\\u2022 Previous experience with emerging technologies like machine learning and AI\\n\\u2022 Experience with visualization tools (e.g. Tableau, PowerBI, Metabase, QLik)\\n\\u2022 Experience working with high data volumes\\n\\u2022 Clear, efficient communication skills\\n\\u2022 Very good English skills\\n\\nDoes this sound like you?\\n\\u2022 Strong analytical thinker\\n\\u2022 Excited about talking about data with others - and good at it too organized and structured\\n\\u2022 Big picture thinker that can also see the small details\\n\\u2022 A big data and analytics nerd\\n\\u2022 Willing to come and work with our fantastic team in Austria\\n\\nJust so you know what to expect: For this position, we offer a competitive overpayment based on qualification and experience. The annual gross salary starts at \\u20ac 46,000 on a full-time basis. If you bring relevant experience in many of the fields of responsibility, the salary will be considerably higher\", \"extensions\": [\"Vor \\u00fcber 1\\u00a0Monat\", \"46.000\\u00a0\\u20ac pro Jahr\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"Vor \\u00fcber 1\\u00a0Monat\", \"schedule_type\": \"Vollzeit\", \"salary\": \"46.000\\u00a0\\u20ac pro Jahr\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCIsImh0aWRvY2lkIjoianFqS1RXTGJHb1ludGk4akFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUhRWFZ6ZEhKcFlRIiwiaGwiOiJkZSIsImZjIjoiRXN3QkNvd0JRVTFzZG5Kd2RtdHJaV2cyVTBSUlltNTRSVGhNWDBvMVNHNW5iVWwwTTJoVVlWVldNMnRFUkdabmFqZHZXVEJHVTJsMU4wSllZa0V5TjJwMFR6RjBPV2xvT0daMVh6SkZSRVJPV1ZSRE5XTTNZbkJmTjI0dFdtc3RVa3RRYlROU2FtSjVVMWN3VlV0MFoxVkVlR1ZUYzJRMU4yZDJNa2x2ZG04NVNsZG9ZbFUwZEZsMWRGSnRSbkE1WTNVU0YzRnZUek5aWm1GRFFXSkhVVGwxT0ZCM04ybDFkVUUwR2lKQlNGZFRUbTFWUjFkdGVISmlXV2hzY21oQ1UyeG1hWGxMTUVOeGJ6SkRkelpCIiwiZmN2IjoiMyIsImZjX2lkIjoiZmNfNDkiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiQmV3ZXJidW5nIGF1ZjogVHJhY3RpdmUiLCJsaW5rIjoiaHR0cHM6Ly90cmFjdGl2ZS5jb20vZW4vamQvZGF0YS1zY2llbnRpc3Q/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5pYyJ9fQ==\"}\n",
      "{\"title\": \"Data Engineer (m/f/d)\", \"company_name\": \"Project A Ventures\", \"location\": \"Koblach, \\u00d6sterreich\", \"via\": \"\\u00fcber JOIN\", \"description\": \"We are looking for:\\n\\nA new team member for one of our international portfolio companies, with the headquarters in Austria. They are leading innovators and full-service providers within their digital commerce industry and they are represented in 90 markets and have 7 offices across the globe.\\n\\nTasks\\n\\u2022 Developing and maintaining the data infrastructure at our portfolio company\\n\\u2022 Expanding their data warehouse to cover further use cases in close collaboration with a team of data architects\\n\\u2022 Responsibility for connecting data to the data warehouse from different sources, such as backends, accounting tools, and many more\\n\\u2022 Ownership of the core analytical data models from a technical perspective\\n\\u2022 Providing input on data models and analytical tasks from a data handling perspective\\n\\nRequirements\\n\\u2022 Background in computer science or a quantitative field with demonstrable coding experience\\n\\u2022 3+ years of experience in a data-related engineering role\\n\\u2022 Proficiency in Python, SQL & Unix\\n\\u2022... Current stack: PostgreSQL & Bigquery, hosted on AWS with Ansible, ETL orchestration in Python (Mara)\\n\\u2022 Advanced SQL skills\\n\\u2022 Ability to work highly independent and deliver projects on time\\n\\u2022 Fluent German and English skills\\n\\u2022 First experiences with regression models, devops & self-service BI tools such as Metabase or Tableau would be desirable\\n\\nBenefits\\n\\u2022 Make a true impact in the commerce industry\\n\\u2022 Take advantage of employee discount opportunities, company benefits, health benefits, and partner programs\\n\\u2022 Thrive from an inspiring, creative, and diverse environment with amazing and smart colleagues from all over the world\\n\\u2022 Transparent culture where every person is highly valued\\n\\u2022 Having fun together as a company with regular team events\\n\\u2022 Getting challenged and developing your professional skills by taking over responsibilities, and a guaranteed steep learning curve from day one\\n\\u2022 Choose from a variety of drinks and healthy snacks available to keep you energized throughout the day\\n\\u2022 Possibility for home office and flexible working hours to balance your work and private life\\n\\u2022 Being part of an instant network within the Project A family\\n\\nDo you want to be part of our portfolio company's success story?\\nWe are looking forward to your online application\", \"thumbnail\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRM_GHeHI08QDJYBH6hSjY7iuwctlVl7KofFfjRuoA&s\", \"extensions\": [\"vor 6 Tagen\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 6 Tagen\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChtL2YvZCkiLCJodGlkb2NpZCI6ImFqaThzX2I2eVR5UHhDNG9BQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lIUVhWemRISnBZUSIsImhsIjoiZGUiLCJmYyI6IkV2Y0JDcmNCUVUxc2RuSndkR3BJWWxjMlUyOTFXV2RYVTBORldIaDViV3MzWlhkSU9VdFdZV1JLYlVobGEzazNkakpLYldseVRFeExTRTlZY1VWU1VWRTNTa0pJY1ZGQkxUSkNaRXc0UXpaaFNpMU5XamRDVUVocGFVMUxaVmhUYzBOelNYbGxOSHBNVUZGSFMyWlZSbXBsVXpCUlZFUnNkRGRHYW5wcFJFeHBSakJ4WmpaNVRrdFNlRkYzWlVKeldFMVZSVjgxTUhaelZWZHBZalJ1UjFkRFduaERWWE5zWlhSbVZtUlNiRmxvZG05WWNqaE9USGx3WmxSVkVoZHhiMDh6V1daaFEwRmlSMUU1ZFRoUWR6ZHBkWFZCTkJvaVFVaFhVMDV0VlMxc1ZtZ3lORUp2U0dnelp6QnlMV3R3VTBod2NtVlRWekZVUVEiLCJmY3YiOiIzIiwiZmNfaWQiOiJmY181NyIsImFwcGx5X2xpbmsiOnsidGl0bGUiOiJCZXdlcmJ1bmcgYXVmOiBKT0lOIiwibGluayI6Imh0dHBzOi8vam9pbi5jb20vY29tcGFuaWVzL3Byb2plY3QtYS8zNDQ5MjIwLWRhdGEtZW5naW5lZXItbS1mLWQ/dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnBpZD1iY2UxZmJkMmYzNjYzOTAwNDZmY1x1MDAyNnV0bV9jYW1wYWlnbj1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9zb3VyY2U9Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fbWVkaXVtPW9yZ2FuaWMifX0=\"}\n",
      "{\"title\": \"Data Lake Solution Architect (m/f/x)\", \"company_name\": \"Raiffeisen Bank International AG Jobportal\", \"location\": \"Wien, \\u00d6sterreich\", \"via\": \"\\u00fcber Raiffeisen Bank International\", \"description\": \"International business requires an international corporate philosophy. Are you open to new ideas and do you value cultural diversity? At Raiffeisen Bank International, we are pleased to have more than 16 million customers in 14 CEE countries. And our journey continues \\u2013 with exciting new issues for us to tackle such as digitalisation and changing customer needs. Join us on our journey.\\n\\nData Lake Solution Architect (m/f/x)\\n\\nWe are looking for a Data Lake Solution Architect who likes to solve difficult problems. As Data Lake Solution Architect you will take over the leadership for designing RBI\\u2019s Data Lake. You will take over an expert role in optimizing the Data Lake for optimal data integration and processing with big data technologies. Additionally, in the journey of professionalized data integration you will be in charge of collaboration across streams to synergize the usage of data within our data eco-system.\\n\\nYour contribution will be a key role for big data integration in the... RBI group across CEE.\\n\\nWhat you can expect:\\n\\u2022 Design infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS \\u2018big data\\u2019 technologies.\\n\\u2022 Identify and design internal process improvements: automating manual processes, source data to optimizing data delivery and re-designing infrastructure for greater scalability.\\n\\u2022 Collaborate across the enterprise to enable and share best practices, reusable and scalable tools and code for our analyst community.\\n\\u2022 Work with data and analytics experts to strive for greater functionality in our data systems.\\n\\u2022 End to End responsibility for tasks from assignment to completion.\\n\\u2022 Strong intercultural skills and delivery orientation combined with the agile and adaptive mind-set are the key success factors in this team.\\n\\nWhat you bring to the table:\\n\\u2022 About 3 years of experience in architectural (best practice) implementation of Data Lake/Data Reservoir eco-systems (AWS Cloud preferred).\\n\\u2022 Experience in design principles and solution finding process how to come from a problem to a solution.\\n\\u2022 Ability to influence senior business stakeholders and your peers to take action / facilitate buy-in of recommendations.\\n\\u2022 Profound experience with CD / DevOps methodology and good overview of related tools or tool chains\\n\\u2022 Proactivity; Curiosity; Responsibility; Ideas & Confidence\\n\\u2022 Fluent knowledge of English; German is appreciated, but not mandatory\\n\\nWhat we offer:\\n\\u2022 You\\u2019ll work in an international team at a leading bank\\n\\u2022 You\\u2019ll benefit from flexible working arrangements and determine your own work-life balance\\n\\u2022 You\\u2019ll benefit from the very latest in tailored professional development\\n\\u2022 You\\u2019ll earn an appropriate salary starting at EUR 47.000 gross p.a. excluding overtime\\n\\nWe are looking forward to receiving you application:\\n\\nhttps://jobs.rbinternational.com\", \"extensions\": [\"Vor \\u00fcber 1\\u00a0Monat\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"Vor \\u00fcber 1\\u00a0Monat\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIExha2UgU29sdXRpb24gQXJjaGl0ZWN0IChtL2YveCkiLCJodGlkb2NpZCI6InlObmxWa2VQdElHYXY5RTFBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lIUVhWemRISnBZUSIsImhsIjoiZGUiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiQmV3ZXJidW5nIGF1ZjogUmFpZmZlaXNlbiBCYW5rIEludGVybmF0aW9uYWwiLCJsaW5rIjoiaHR0cHM6Ly9qb2JzLnJiaW50ZXJuYXRpb25hbC5jb20vRGF0YS1MYWtlLVNvbHV0aW9uLUFyY2hpdGVjdC1tZngtZW5nLWo1NTIzLmh0bWw/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5pYyJ9fQ==\"}\n",
      "{\"title\": \"Data Scientist (f/m/x)\", \"company_name\": \"Global Finance\", \"location\": \"Wien, \\u00d6sterreich\", \"via\": \"\\u00fcber Takeda Careers\", \"description\": \"By clicking the \\u201cApply\\u201d button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda\\u2019s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.\\n\\nJob Description\\n\\nAre you looking for a patient-focused, innovative-driven company that will inspire you and empower you to shine? Join us as our Data Scientist in our office in Vienna, Austria.\\n\\nHere, you will be a vital contributor to our inspiring, bold mission.\\n\\nAs our successful Data Scientist, you will collaborate with business users in different functions including Marketing, Operations and Supply Chain/Manufacturing to understand the current state and identify opportunities to transform the business into a data-driven organization.\\n\\nYou will translate processes, and requirements into analytics solutions and metrics with effective data... strategy, data quality, and data accessibility for decision making. Operationalize decision support solutions and drive use adoption as well as gathering feedback and metrics on Voice of Customer to improve analytics services.\\n\\nYou will understand and apply the appropriate quantitative techniques to provide businesses with actionable insights and ensure analytics model and data are access to the end users to evaluate \\u201cwhat-if\\u201d scenarios and decision making.\\n\\nYou will evaluate the data, analytical models, and experiments periodically to validate hypothesis ensuring it continues to provide business value as requirements and objectives evolve.\\n\\nYour New Opportunity:\\n\\u2022 Collaborate with business partners in identifying analytical opportunities and developing BI-related goals and projects that will create strategically relevant insights.\\n\\u2022 Work with internal and external partners to develop analytics vision and programs to advance BI solutions and practices.\\n\\u2022 Understand data and sources of data. Strategize with IT development team and develop a process to collect, ingest, and deliver data along with proper data models for analytical needs.\\n\\u2022 Interact with business users to define pain points, problem statement, scope, and analytics business case. Develop solutions with recommended data model and business intelligence technologies including data warehouse, data marts, OLAP modelling, dashboards/reporting, and data queries.\\n\\u2022 Work with DevOp and database teams to ensure proper design of system databases and appropriate integration with other enterprise applications.\\n\\u2022 Collaborate with Enterprise Data and Analytics Team to design data model and visualization solutions that synthesize complex data for data mining and discovery.\\n\\u2022 Assist in defining requirements and facilitates workshops and prototyping sessions.\\n\\u2022 Develop and applies technologies such as machine-learning, deep-learning algorithm to enable advanced analytics product functionality.\\n\\nYour Skills and Qualifications:\\n\\u2022 Bachelors\\u2019 Degree, from an accredited institution in Data Science, Statistics, Computer Science, or related field.\\n\\u2022 3 - 5 years of experience in BI application development, and experience with data warehousing and reporting tools.\\n\\u2022 Experience and proficiency with BI and Analytics tools Qlik, Power BI, ETL, NoSQL/Graph databases, and Hadoop Platform with sharp analytical abilities and proven design skills.\\n\\u2022 A deep familiarity with analytical solutions including BI/Reporting and Dashboards.\\n\\u2022 Experience with statistical modelling such as clustering, segmentation, multivariate, regression, etc. and analytics tools such as R, Python, Databricks, SageMaker, etc. a plus.\\n\\u2022 Experience in developing and applying predictive and prescriptive modelling, deep-learning, or other machine learning techniques a plus.\\n\\u2022 Well-developed organizational skills and the ability to manage multiple projects simultaneously.\\n\\u2022 Excellent written and verbal communication skills including the ability to interact effectively with multifunctional teams. Demonstrated aptitude for collaboration with other strategic partners.\\n\\u2022 Solid understanding of current and emerging BI/Advanced Analytics technologies. Expertise in the areas of Data, Data Modelling; Statistical Methods, and AI/ML preferred.\\n\\u2022 Familiarity with Analytics processes and service models.\\n\\nAt Takeda, we are transforming the pharmaceutical industry through our R&D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Takeda encourages and supports its employees by trainings, job rotations and mentoring. A balanced work \\u2013 life ratio (recognition for work and family, company kindergarten) and numerous benefits (fitness center, cafeteria, etc.) complete our profile as a top employer.\\n\\nWe foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver Better Health and a Brighter Future to people around the world. Gender, age, skin color, background or sexual orientation play no role in this as we actively promote diversity. We equally address people with disabilities. In the course of a recruitment process which is as free of barriers as possible, we encourage you to mention all relevant information in this context in your application.\\n\\nMotivated employees must be remunerated appropriately. The minimum salary for this important and responsible position is \\u20ac3.389,68 gross per month (full time, collective wage agreement for the chemical industry). The actual remuneration package will be guided by your professional experience and your qualifications, so increased payment is possible.\\n\\nFor more insights into Takeda click here.\\n\\nLocations\\n\\nAUT - Wien - DC Tower\\n\\nWorker Type\\n\\nEmployee\\n\\nWorker Sub-Type\\n\\nRegular\\n\\nTime Type\\n\\nFull time\", \"extensions\": [\"vor 22 Tagen\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 22 Tagen\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAoZi9tL3gpIiwiaHRpZG9jaWQiOiJ2TkxKTWxVU29jMDQwcml3QUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJSFFYVnpkSEpwWVEiLCJobCI6ImRlIiwiYXBwbHlfbGluayI6eyJ0aXRsZSI6IkJld2VyYnVuZyBhdWY6IFRha2VkYSBDYXJlZXJzIiwibGluayI6Imh0dHBzOi8vd3d3LnRha2VkYWpvYnMuY29tL2pvYi92aWVubmEvZGF0YS1zY2llbnRpc3QtZi1tLXgvMTExMy8xODY3NTk5MTE2OD91dG1fY2FtcGFpZ249Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fc291cmNlPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX21lZGl1bT1vcmdhbmljIn19\"}\n",
      "{\"title\": \"Data Scientist (m/f/x)\", \"company_name\": \"Zumtobel Group AG\", \"location\": \"\\u00d6sterreich\", \"via\": \"\\u00fcber Www.laendlejob.at\", \"description\": \"The Zumtobel Group is an international lighting group and a leading supplier of innovative lighting solutions, lighting components and associated services. With its core brands, Zumtobel, Thorn and Tridonic, the Group offers its customers around the world a comprehensive portfolio of products and services.\\n\\nYour job\\n\\nIn this exciting position, you will be responsible for applying and developing statistical methods to solve specific business problems.\\n\\u2022 You analyze complex data and evaluate them using analysis models and algorithms.\\n\\u2022 You will also develop new data sources, test model improvements, and ensure improvement of existing methods by tuning model parameters.\\n\\u2022 You are actively involved in the creation and development of concepts for automated reporting and forecasting systems.\\n\\u2022 You will recognize and identify data patterns and gain meaningful and actionable insights through analysis.\\n\\u2022 This is how you support forward-looking business decisions and optimize existing... processes.\\n\\u2022 You understand business processes and are able to back them up with data and create derivations from the data.\\n\\u2022 With your outstanding analytical flair and understanding, you are able to define metrics and analytics models to build an enterprise-wide analytics system.\\n\\nYour profile\\n\\nYou are a digital native with a strong affinity for numbers, data and facts. In addition, they are characterized by their eagerness and willingness to structure data and to read and interpret it at different levels.\\n\\u2022 You have a degree in computer science, business informatics, business administration with a focus on data analysis or a similar specialization.\\n\\u2022 You have already been able to get a taste of professional life in a relevant work environment.\\n\\u2022 Personally, you convince with your independent and proactive working style. You have a strong analytical mindset and dare to make recommendations and suggestions for improvement.\\n\\u2022 You are driven by innovation, recognize the potential of important business processes and drive them forward on your own initiative and independently.\\n\\u2022 You like to think outside the box and are open to new processes, ideas and analytical methods.\\n\\u2022 You use your communication skills in English to your advantage when communicating with international colleagues and external stakeholders. German language skills are an advantage.\\n\\u2022 Sound knowledge of SAP as well as cloud computing solutions (AWS, GCP and IBM) round off your profile.\\n\\nLegally binding notice:\\n\\nBased on the minimum salary stipulated in the collective agreement of \\u20ac 47.573,96 p.a. (for a 38,50-hour working week), the actual salary for this position is based on professional qualifications and experience.\\n\\nAt the Zumtobel Group, we not only understand the power of the visible, but also the invisible. Like light itself, diversity cannot be grasped but it can be perceived. We appreciate the uniqueness of each individual and recognise it as a driver of innovation. Qualified applicants will receive consideration without regard to e.g.: race, colour, sex, religion, age, sexual orientation, gender identity/expression, or disability\", \"thumbnail\": \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT0UlNZpAvT8q8t947Zny2M9frPIa9FtmWZZzeYKVXQ0GLe6LuW2B34XQ&s\", \"extensions\": [\"vor 7 Tagen\", \"Vollzeit\"], \"detected_extensions\": {\"posted_at\": \"vor 7 Tagen\", \"schedule_type\": \"Vollzeit\"}, \"job_id\": \"eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAobS9mL3gpIiwiaHRpZG9jaWQiOiJITnlrcFNqTlJKQnZFNUVWQUFBQUFBPT0iLCJ1dWxlIjoidytDQUlRSUNJSFFYVnpkSEpwWVEiLCJobCI6ImRlIiwiYXBwbHlfbGluayI6eyJ0aXRsZSI6IkJld2VyYnVuZyBhdWY6IFd3dy5sYWVuZGxlam9iLmF0IiwibGluayI6Imh0dHBzOi8vd3d3LmxhZW5kbGVqb2IuYXQvZGF0YS1zY2llbnRpc3QtbWZ4LzI0NTk3NzIvai5odG1sP3V0bV9jYW1wYWlnbj1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9zb3VyY2U9Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fbWVkaXVtPW9yZ2FuaWMifX0=\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13974/3155062777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmycol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    print('{} added to {}'.format(message, collection))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mnext_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mrecord_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;31m# Generators are stateful, and it is possible that the tp / records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_next_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;31m# after the long poll, we should check whether the group needs to rebalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# prior to returning data so that the group can stabilize faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# avoid negative timeouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;31m# called without the lock to avoid deadlock potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    m = message.value\n",
    "    print(m)\n",
    "    mycol.insert_one(loads(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd683e8-5bce-4f1d-a594-eb33f2a1271f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
